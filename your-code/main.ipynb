{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_10288\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
       "1                                           Will do.      0\n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
       "4                                                fyi      0"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29     ----------- REGARDS, MR NELSON SMITH.KINDLY RE...\n",
       "535    I have not been able to reach oscar this am. W...\n",
       "695    ; Huma Abedin B6I'm checking with Pat on the 5...\n",
       "557    I can have it announced here on Monday - can't...\n",
       "836        BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<[^<]+?>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'\\b\\w', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove prefixed 'b'\n",
    "    text = text.replace('b', '')\n",
    "    \n",
    "    # Convert to Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "# Apply the preprocessing function to the text column\n",
    "\n",
    "X_train = X_train.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29      egards r elson mithkindly eply e n y rivate m...\n",
       "535     ave ot een le o each scar his m e re upposed ...\n",
       "695     uma edin im hecking ith at n he ill ork ith a...\n",
       "557               an ave t nnounced ere n onday ant oday\n",
       "836     ank f fricaagence an edro p an edro ote ivoir...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     egards r elson mithkindly eply e n rivate mail...\n",
      "535              ave ot een le scar e upposed end eceive\n",
      "695    uma edin im hecking ith n ill ork ith ack ake ...\n",
      "557                    ave nnounced ere n onday ant oday\n",
      "836    ank f fricaagence edro p edro ote ivoirewest f...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "X_train = X_train.apply(remove_stopwords)\n",
    "X_test = X_test.apply(remove_stopwords)\n",
    "\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29     egards r elson mithkindly eply e n rivate mail...\n",
       "535              ave ot een le scar e upposed end eceive\n",
       "695    uma edin im hecking ith n ill ork ith ack ake ...\n",
       "557                    ave nnounced ere n onday ant oday\n",
       "836    ank f fricaagence edro p edro ote ivoirewest f...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "X_train = X_train.apply(lemmatize_text)\n",
    "\n",
    "X_test = X_test.apply(lemmatize_text)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in HAM messages:\n",
      "n: 917\n",
      "f: 775\n",
      "nd: 761\n",
      "e: 526\n",
      "hat: 420\n",
      "ou: 248\n",
      "ith: 206\n",
      "ut: 198\n",
      "ill: 195\n",
      "r: 194\n",
      "\n",
      "Top 10 words in SPAM messages:\n",
      "f: 4310\n",
      "n: 3657\n",
      "e: 3346\n",
      "nd: 3342\n",
      "ou: 2467\n",
      "ill: 1512\n",
      "hat: 1455\n",
      "ith: 1043\n",
      "ave: 805\n",
      "oney: 795\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Configure the CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words='english',  # Remove English stop words\n",
    "    lowercase=True,        # Convert text to lowercase\n",
    "    token_pattern=r'\\b[a-zA-Z]+\\b',  # Only consider words with letters\n",
    "    max_features=1000      # Limit the number of features\n",
    ")\n",
    "\n",
    "# Transform the text into a Bag of Words (BoW) matrix\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Separate ham and spam messages\n",
    "ham_indices = y_train == 0  # Assuming ham is labeled as 0\n",
    "spam_indices = y_train == 1  # Assuming spam is labeled as 1\n",
    "\n",
    "X_train_ham_bow = X_train_bow[ham_indices]  # BoW for ham messages\n",
    "X_train_spam_bow = X_train_bow[spam_indices]  # BoW for spam messages\n",
    "\n",
    "# Function to get the top N words\n",
    "def get_top_words(bow_features, feature_names, n=10):\n",
    "    # Sum the word counts along axis 0 (columns)\n",
    "    word_counts = np.asarray(bow_features.sum(axis=0)).flatten()\n",
    "    # Create a list of (word, count) tuples\n",
    "    word_counts = list(zip(feature_names, word_counts))\n",
    "    # Sort the list by count in descending order and select the top N words\n",
    "    sorted_words = sorted(word_counts, key=lambda x: x[1], reverse=True)[:n]\n",
    "    return sorted_words\n",
    "\n",
    "# Get and print the top 10 words for ham and spam\n",
    "top_ham_words = get_top_words(X_train_ham_bow, feature_names, n=10)\n",
    "top_spam_words = get_top_words(X_train_spam_bow, feature_names, n=10)\n",
    "\n",
    "print(\"Top 10 words in HAM messages:\")\n",
    "for word, count in top_ham_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 words in SPAM messages:\")\n",
    "for word, count in top_spam_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29     egards r elson mithkindly eply e n rivate mail...\n",
       "535              ave ot een le scar e upposed end eceive\n",
       "695    uma edin im hecking ith n ill ork ith ack ake ...\n",
       "557                    ave nnounced ere n onday ant oday\n",
       "836    ank f fricaagence edro p edro ote ivoirewest f...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>egards r elson mithkindly eply e n rivate mail...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>ave ot een le scar e upposed end eceive</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>uma edin im hecking ith n ill ork ith ack ake ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>ave nnounced ere n onday ant oday</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>ank f fricaagence edro p edro ote ivoirewest f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  money_mark  \\\n",
       "29   egards r elson mithkindly eply e n rivate mail...           1   \n",
       "535            ave ot een le scar e upposed end eceive           1   \n",
       "695  uma edin im hecking ith n ill ork ith ack ake ...           1   \n",
       "557                  ave nnounced ere n onday ant oday           1   \n",
       "836  ank f fricaagence edro p edro ote ivoirewest f...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "29                  0        72  \n",
       "535                 0        39  \n",
       "695                 0        82  \n",
       "557                 0        33  \n",
       "836                 0      1094  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "\n",
    "# Convert X_train and X_test to DataFrames\n",
    "X_train = pd.DataFrame(X_train, columns=['text'])\n",
    "X_test = pd.DataFrame(X_test, columns=['text'])\n",
    "\n",
    "# List of currency symbols and suspicious words\n",
    "money_symbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", \"$\"])\n",
    "suspicious_words = \"|\".join([\"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"])\n",
    "\n",
    "# Add indicators to the training DataFrame\n",
    "X_train['money_mark'] = X_train['text'].str.contains(money_symbol_list, case=False, regex=True) * 1\n",
    "X_train['suspicious_words'] = X_train['text'].str.contains(suspicious_words, case=False, regex=True) * 1\n",
    "X_train['text_len'] = X_train['text'].apply(lambda x: len(x))\n",
    "\n",
    "# Add indicators to the test DataFrame\n",
    "X_test['money_mark'] = X_test['text'].str.contains(money_symbol_list, case=False, regex=True) * 1\n",
    "X_test['suspicious_words'] = X_test['text'].str.contains(suspicious_words, case=False, regex=True) * 1\n",
    "X_test['text_len'] = X_test['text'].apply(lambda x: len(x))\n",
    "\n",
    "# Display the first few rows of the training DataFrame\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would you create a Bag of Words with the CountVectorizer method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW matrix shape: (800, 1000)\n",
      "Top 10 most frequent words:\n",
      "f: 5085\n",
      "\n",
      "n: 4574\n",
      "\n",
      "nd: 4103\n",
      "\n",
      "e: 3872\n",
      "\n",
      "ou: 2715\n",
      "\n",
      "hat: 1875\n",
      "\n",
      "ill: 1707\n",
      "\n",
      "ith: 1249\n",
      "\n",
      "ave: 978\n",
      "\n",
      "r: 868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Configure the CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words='english',  # Remove English stop words\n",
    "    lowercase=True,        # Convert text to lowercase\n",
    "    token_pattern=r'\\b[a-zA-Z]+\\b',  # Only consider words with letters\n",
    "    max_features=1000      # Limit the number of features\n",
    ")\n",
    "\n",
    "# Transform the text into a Bag of Words (BoW) matrix\n",
    "\n",
    "X_train_bow = vectorizer.fit_transform(X_train['text'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"BoW matrix shape:\", X_train_bow.shape)\n",
    "\n",
    "print(\"Top 10 most frequent words:\")\n",
    "\n",
    "top_words = get_top_words(X_train_bow, feature_names, n=10)\n",
    "\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (800, 1000)\n",
      "Top 10 most frequent words:\n",
      "f: 119.50810507439287\n",
      "\n",
      "n: 113.92349649248122\n",
      "\n",
      "e: 107.81892920087421\n",
      "\n",
      "nd: 101.39581837876162\n",
      "\n",
      "ou: 79.50878441793752\n",
      "\n",
      "ill: 56.68349110129105\n",
      "\n",
      "hat: 49.438459737047225\n",
      "\n",
      "ith: 37.310789811430766\n",
      "\n",
      "yi: 33.127904855234725\n",
      "\n",
      "ave: 31.758954257137198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Configure the TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',  # Remove English stop words\n",
    "    lowercase=True,        # Convert text to lowercase\n",
    "    token_pattern=r'\\b[a-zA-Z]+\\b',  # Only consider words with letters\n",
    "    max_features=1000      # Limit the number of features\n",
    ")\n",
    "\n",
    "# Vectorize the text into a TF-IDF matrix\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['text'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", X_train_tfidf.shape)\n",
    "\n",
    "print(\"Top 10 most frequent words:\")\n",
    "\n",
    "top_words = get_top_words(X_train_tfidf, feature_names, n=10)\n",
    "\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task (optional) - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "Use a MultinimialNB with default parameters.\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98       125\n",
      "           1       0.93      1.00      0.96        75\n",
      "\n",
      "    accuracy                           0.97       200\n",
      "   macro avg       0.96      0.98      0.97       200\n",
      "weighted avg       0.97      0.97      0.97       200\n",
      "\n",
      "Confusion Matrix:\n",
      "[[119   6]\n",
      " [  0  75]]\n",
      "Cross-Validation Accuracy Scores: [0.965 0.96  0.97  0.97  0.945]\n",
      "Mean CV Accuracy: 0.962\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "model = MultinomialNB()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "cv_scores = cross_val_score(model, X, data['label'], cv=5)\n",
    "print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
